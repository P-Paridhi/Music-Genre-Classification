# -*- coding: utf-8 -*-
"""Genre_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X_cJGEDypACWcnckLP9aH3wI4uvvKcnb
"""

!pip install pydub

import tensorflow as tf
import numpy as np
import scipy
from scipy import misc
import glob
from PIL import Image
import os
import matplotlib.pyplot as plt
import librosa
from keras import layers
from keras.layers import (Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, 
                          Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D)
from keras.models import Model, load_model
from keras.preprocessing import image
from keras.utils import layer_utils
import pydot
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
from keras.utils import plot_model
from keras.optimizers import Adam
from keras.initializers import glorot_uniform
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from pydub import AudioSegment
import shutil
from keras.preprocessing.image import ImageDataGenerator
import random

genres = 'blues classical country disco pop hiphop metal reggae rock'
genres = genres.split()
for g in genres:
  path_audio = os.path.join('/content/audio3sec',f'{g}')
  os.makedirs(path_audio)

from pydub import AudioSegment
newSong = 'newSong'
i = 0
for g in genres:
  j=0
  print(f"{g}")
  for filename in os.listdir(os.path.join('/content/drive/MyDrive/GTZAN/Data/genres_original',f"{g}")):

    song  =  os.path.join(f'/content/drive/MyDrive/GTZAN/Data/genres_original/{g}',f'{filename}')
    j = j+1
    for w in range(0,10):
      i = i+1
      #print(i)
      t1 = 3*(w)*1000
      t2 = 3*(w+1)*1000
      newAudio = AudioSegment.from_wav(song)
      new = newAudio[t1:t2]
      new.export(f'/content/audio3sec/{g}/{g+str(j)+str(w)}.wav', format="wav")

!zip -r /content/audio3sec.zip /content/audio3sec



data_path = '/content/audio3sec'

# List of genres in the dataset
genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'metal', 'pop', 'reggae', 'rock']

# Define the number of samples per audio file
samples_per_track = 22050 * 3  # 30 seconds of audio

# Define the number of MFCC coefficients
n_mfcc = 20

# Define the size of the input shape for the model
input_shape = (n_mfcc, samples_per_track // 512, 1)

def extract_features(file_path):
    signal, sr = librosa.load(file_path, sr=22050)
    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)
    if mfccs.shape[1] >= input_shape[1]:
        mfccs = mfccs[:, :input_shape[1]]
    else:
        pad_width = input_shape[1] - mfccs.shape[1]
        mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')
    return mfccs.reshape(input_shape)

from keras.utils import to_categorical
# Create the dataset
def create_dataset(data_path):
    X = []
    y = []
    for genre_idx, genre in enumerate(genres):
        genre_path = os.path.join(data_path, genre)
        for file_name in os.listdir(genre_path):
            file_path = os.path.join(genre_path, file_name)
            features = extract_features(file_path)
            X.append(features)
            y.append(to_categorical(genre_idx, num_classes=len(genres)))
    return np.array(X), np.array(y)

X, y = create_dataset(data_path)

from sklearn.model_selection import train_test_split
# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.models import Sequential
# Define the model
model = Sequential()
model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(16, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))


model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(genres), activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint 
# Define the model checkpoint
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, save_weights_only=False, monitor='val_accuracy', mode='max', verbose=1)

# Train the model
history = model.fit(X_train, y_train,
                batch_size=32,
                epochs=100,
                validation_data=(X_val, y_val),
                callbacks=[model_checkpoint])

model.save('/content/drive/MyDrive/GTZAN/best_model.h5')

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Get predictions for the validation set
y_pred = model.predict(X_val)

# Convert one-hot encoded labels back to class labels
y_true = np.argmax(y_val, axis=1)
y_pred = np.argmax(y_pred, axis=1)

# Calculate the confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Plot the confusion matrix
sns.heatmap(cm, annot=True, fmt='g', xticklabels=genres, yticklabels=genres)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

import librosa.display

# Load the trained model
model = load_model('best_model.h5')

# Define the path of the test music piece
test_music_path = '/content/Taylor Swift-Love Story.mp3'

# Extract the MFCC features of the test music piece
test_music_features = extract_features(test_music_path)

# Reshape the features to match the input shape of the model
test_music_features = test_music_features.reshape(1, *input_shape)

# Use the model to predict the genre probabilities for the test music piece
genre_probabilities = model.predict(test_music_features)[0]

# Plot the genre probabilities
plt.figure(figsize=(10,5))
plt.bar(genres, genre_probabilities)
plt.title('Genre Probabilities')
plt.xlabel('Genre')
plt.ylabel('Probability')
plt.show()

# Display the spectrogram of the test music piece
y, sr = librosa.load(test_music_path, duration=30)
librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max),
                         y_axis='log', x_axis='time')
plt.title('Spectrogram')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()
plt.show()

import librosa.display

# Load the trained model
model = load_model('best_model.h5')

# Define the path of the test music piece
test_music_path = '/content/Nirvana-Smells Like Teen Spirit.mp3'

# Extract the MFCC features of the test music piece
test_music_features = extract_features(test_music_path)

# Reshape the features to match the input shape of the model
test_music_features = test_music_features.reshape(1, *input_shape)

# Use the model to predict the genre probabilities for the test music piece
genre_probabilities = model.predict(test_music_features)[0]

# Plot the genre probabilities
plt.figure(figsize=(10,5))
plt.bar(genres, genre_probabilities)
plt.title('Genre Probabilities')
plt.xlabel('Genre')
plt.ylabel('Probability')
plt.show()

# Display the spectrogram of the test music piece
y, sr = librosa.load(test_music_path, duration=30)
librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max),
                         y_axis='log', x_axis='time')
plt.title('Spectrogram')
plt.colorbar(format='%+2.0f dB')
plt.tight_layout()
plt.show()